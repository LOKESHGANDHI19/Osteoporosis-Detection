# -*- coding: utf-8 -*-
"""original osteoporosis-knee-xray.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aoMnUfgWT7ul3Lc5MUcnmMb-w5_rqTjK
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.pyplot import imshow
import seaborn as sns
sns.set_style('darkgrid')

from PIL import Image

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score
from tensorflow.keras import backend as K
from tensorflow.keras.optimizers import Adam, Adamax
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf # Imports tensorflow


from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras import layers
from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization,Embedding,TimeDistributed
from tensorflow.keras.layers import Conv2D, MaxPooling2D, ReLU, LSTM,Bidirectional,Attention,Concatenate,concatenate
from tensorflow.keras import regularizers, optimizers,losses
from tensorflow.keras.layers import DepthwiseConv2D,Add, ReLU, GlobalAveragePooling2D, GlobalMaxPooling2D,MultiHeadAttention
from tensorflow.keras.layers import Activation,ActivityRegularization, AvgPool2D, LeakyReLU, Conv2DTranspose
from tensorflow.keras.metrics import Accuracy,Recall,Precision,AUC,TruePositives,TrueNegatives,FalseNegatives,FalsePositives, SpecificityAtSensitivity,SensitivityAtSpecificity
from tensorflow.keras.utils import plot_model
from tensorflow.keras.preprocessing import image
from tensorflow.keras.utils import to_categorical
from tensorflow.python.keras.utils import np_utils
import numpy as np
import pandas as pd
import matplotlib
import seaborn as sns
import sklearn

import matplotlib.pyplot as plt
import time
import os
import sklearn.metrics as m
from glob import glob
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn import tree
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
import skimage.io
import skimage.color
import skimage.filters
import os

from google.colab import drive
drive.mount('/content/drive')

filepaths = []
labels = []
osteo_dir = r'/content/drive/MyDrive/osteoporosis-knee-xray/osteoporosis'
normal_dir = r'/content/drive/MyDrive/osteoporosis-knee-xray/normal'

for d in [osteo_dir, normal_dir]:
    flist=os.listdir(d)

    for f in flist:
        fpath=os.path.join(d,f)
        filepaths.append(fpath)

        if d == osteo_dir:
            labels.append('Osteoporosis')
        else:
            labels.append('Normal')

print ('Total number of images: ', len(labels))

Fseries = pd.Series(filepaths, name='filepaths')
Lseries = pd.Series(labels, name='labels')

df = pd.concat([Fseries, Lseries], axis=1)
df.head()

df['labels'].value_counts()

height=200
width=400
channels=3
batch_size=80
img_shape=(height, width, channels)
img_size=(height, width)
train_split=.9
test_split=.05

dummy_split = test_split/(1-train_split)
train_df, dummy_df = train_test_split(df, train_size = train_split, shuffle = True, random_state = 123)
test_df, valid_df = train_test_split(dummy_df,
                                     train_size = dummy_split,
                                     shuffle = True,
                                     random_state = 123)
print ('train samples: ', len(train_df), '  test samples: ', len(test_df), ' validation samples', len(valid_df))

length = len(test_df)
test_batch_size = sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and length/n<=80],reverse=True)[0]

test_steps = int(length/test_batch_size)
print ('test batch size: ' ,test_batch_size, '  test steps: ', test_steps)

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest')

train_gen = train_datagen.flow_from_dataframe(
    train_df,
    x_col='filepaths',
    y_col='labels',
    target_size=img_size,
    class_mode='categorical',
    color_mode='rgb',
    shuffle=False,
    batch_size=batch_size)

test_datagen = ImageDataGenerator(rescale=1./255)

test_gen = test_datagen.flow_from_dataframe(
    test_df,
    x_col='filepaths',
    y_col='labels',
    target_size=img_size,
    class_mode='categorical',
    color_mode='rgb',
    shuffle=False,
    batch_size=test_batch_size)

valid_datagen = ImageDataGenerator(rescale=1./255)

valid_gen = valid_datagen.flow_from_dataframe(
    valid_df,
    x_col='filepaths',
    y_col='labels',
    target_size=img_size,
    class_mode='categorical',
    color_mode='rgb',
    shuffle=False,
    batch_size=batch_size)

classes = list(train_gen.class_indices.keys())
class_count = len(classes)
train_steps = int(len(train_gen.labels)/batch_size)

# Define the model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=img_shape),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(class_count, activation='sigmoid')
])

# Compile the model
model.compile(
    optimizer = Adam(),  # Use the Adam optimizer
    loss = "binary_crossentropy",  # Use categorical cross-entropy loss for multi-class classification
    metrics = ["accuracy"]  # Monitor the accuracy during training
)

# Train the model
history = model.fit(
    train_gen,  # Pass your training data
    validation_data = valid_gen,
    epochs=10,  # Set the number of epochs to train for
)

# Extract training and validation loss and accuracy from history
train_loss = history.history['loss']
train_acc = history.history['accuracy']
val_loss = history.history['val_loss']
val_acc = history.history['val_accuracy']

# Find the index of the epoch with the best validation accuracy
best_epoch = val_acc.index(max(val_acc))

# Plot training and validation loss
plt.figure(figsize=(10, 5))
plt.plot(train_loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.title('Base Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plot training and validation accuracy
plt.figure(figsize=(10, 5))
plt.plot(train_acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.title('Base Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')

# Highlight the overall best epoch
best_val_acc = max(val_acc)
plt.axvline(x=val_acc.index(best_val_acc), color='blue', linestyle='--', label=f'Best Epoch ({best_val_acc:.4f})')

plt.legend()
plt.show()

y_pred = model.predict(test_gen, steps=test_steps).argmax(axis=1)
y_true = test_gen.classes

weighted_f1_score = f1_score(y_true, y_pred, average='weighted')
accuracy = accuracy_score(y_true, y_pred)

# Print the results
print('Weighted F1 score of both classes:', weighted_f1_score)
print('Accuracy of the model:', accuracy)

report = classification_report(y_true, y_pred)
print("Classification Report:\n----------------------\n", report)

# Compute the confusion matrix
conf_mat = confusion_matrix(y_true, y_pred)

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(8, 8))
sns.heatmap(conf_mat, annot=True, cmap='Blues')
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.show()

from tensorflow.keras.applications.vgg19 import VGG19
from tensorflow.keras.applications.nasnet import NASNetMobile

from tensorflow.keras.applications.resnet_v2 import ResNet50V2
from tensorflow.keras.applications.resnet_rs import ResNetRS50

from tensorflow.keras.applications.efficientnet_v2 import EfficientNetV2B1
from tensorflow.keras.applications.inception_v3 import InceptionV3

classifier = ResNet50V2(
            include_top = False,input_shape=img_shape,
             weights='imagenet'
           )
fine_tune_at = 180
for layer in classifier.layers[:fine_tune_at]:
    layer.trainable = False

Name='ResNet50V2'
model = Sequential()
model.add(classifier)
model.add(GlobalAveragePooling2D())
model.add(Dense(256, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.25))
model.add(Dense(32, activation='sigmoid'))
model.add(Dropout(0.25))
model.add(Dense(class_count, activation='sigmoid'))
print(model.summary())

# Compile the model
model.compile(
    optimizer= Adamax(),  # Use the Adam optimizer
    loss="binary_crossentropy",  # Use categorical cross-entropy loss for multi-class classification
    metrics=["accuracy",Recall(),Precision(),AUC(),TruePositives(),TrueNegatives(),FalseNegatives(),FalsePositives()]  # Monitor the accuracy during training
)

# Train the model
history = model.fit(
    train_gen,  # Pass your training data
    validation_data = valid_gen,
    epochs=100,  # Set the number of epochs to train for
)

pd.DataFrame.from_dict(history.history).to_csv(Name+'.csv',index=False)

# Extract training and validation loss and accuracy from history
train_loss = history.history['loss']
train_acc = history.history['accuracy']
val_loss = history.history['val_loss']
val_acc = history.history['val_accuracy']

# Find the index of the epoch with the best validation accuracy
best_epoch = val_acc.index(max(val_acc))

# Plot training and validation loss
plt.figure(figsize=(10, 5))
plt.plot(train_loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.title('InceptionResNetV2 Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plot training and validation accuracy
plt.figure(figsize=(10, 5))
plt.plot(train_acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.title('InceptionResNetV2 Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')

# Highlight the overall best epoch
best_val_acc = max(val_acc)
plt.axvline(x=val_acc.index(best_val_acc), color='blue', linestyle='--', label=f'Best Epoch ({best_val_acc:.4f})')

plt.legend()
plt.show()

y_pred = model.predict(test_gen, steps=test_steps).argmax(axis=1)
y_true = test_gen.classes

weighted_f1_score = f1_score(y_true, y_pred, average='weighted')
accuracy = accuracy_score(y_true, y_pred)

# Print the results
print('InceptionResNetV2 Model Weighted F1 score of both classes:', weighted_f1_score)
print('InceptionResNetV2 Model Accuracy of the model:', accuracy)

report = classification_report(y_true, y_pred)
print("Classification Report:\n----------------------\n", report)

g# Compute the confusion matrix
conf_mat = confusion_matrix(y_true, y_pred)

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(8, 8))
sns.heatmap(conf_mat, annot=True, cmap='Blues')
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.show()